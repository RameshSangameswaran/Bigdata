{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2548ae94-8baa-48b8-98d4-8393dc2d31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e00406-78c6-4615-9aa8-4fb9fcd16e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.50-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pandas>=1.3.0 (from yfinance)\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.16.5 (from yfinance)\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: requests>=2.31 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lxml>=4.9.1 (from yfinance)\n",
      "  Downloading lxml-5.3.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from yfinance) (2024.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.17.8.tar.gz (948 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.2/948.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from yfinance) (4.12.3)\n",
      "Collecting html5lib>=1.1 (from yfinance)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.3.0->yfinance)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
      "Downloading yfinance-0.2.50-py2.py3-none-any.whl (102 kB)\n",
      "Downloading frozendict-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.8-cp39-cp39-linux_x86_64.whl size=311594 sha256=9427b166032bdca0d0b8b09b15e0735d7b6763341ace90d3e4454088fab8e268\n",
      "  Stored in directory: /home/hadoop/.cache/pip/wheels/06/b3/7f/ed42a7c83ad89f578928833f5789212c694a015b8bd6a407a1\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, tzdata, numpy, lxml, html5lib, frozendict, pandas, yfinance\n",
      "Successfully installed frozendict-2.4.6 html5lib-1.1 lxml-5.3.0 multitasking-0.0.11 numpy-2.0.2 pandas-2.2.3 peewee-3.17.8 tzdata-2024.2 yfinance-0.2.50\n"
     ]
    }
   ],
   "source": [
    "! pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78dfb7f-db39-4b8c-8005-4a23c9792c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "data = yf.download(\"AAPL IBM\", start=\"2009-01-01\", end=\"2019-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12eab41-43a1-4362-81cd-660997ee2f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>IBM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>3.067143</td>\n",
       "      <td>80.200768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>3.327500</td>\n",
       "      <td>82.619499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>3.426786</td>\n",
       "      <td>83.279160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>3.278929</td>\n",
       "      <td>83.967499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>3.229643</td>\n",
       "      <td>83.948372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>70.132500</td>\n",
       "      <td>129.808792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>71.172501</td>\n",
       "      <td>129.646271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>71.205002</td>\n",
       "      <td>129.043976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>72.779999</td>\n",
       "      <td>129.063095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>72.364998</td>\n",
       "      <td>129.254303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2767 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker           AAPL         IBM\n",
       "Date                             \n",
       "2009-01-02   3.067143   80.200768\n",
       "2009-01-05   3.327500   82.619499\n",
       "2009-01-06   3.426786   83.279160\n",
       "2009-01-07   3.278929   83.967499\n",
       "2009-01-08   3.229643   83.948372\n",
       "...               ...         ...\n",
       "2019-12-23  70.132500  129.808792\n",
       "2019-12-24  71.172501  129.646271\n",
       "2019-12-26  71.205002  129.043976\n",
       "2019-12-27  72.779999  129.063095\n",
       "2019-12-30  72.364998  129.254303\n",
       "\n",
       "[2767 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c948336e-0ddd-4523-b53b-ed9642d5d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [nmit]\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "$HADOOP_HOME/sbin/start-dfs.sh\n",
    "$HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a16cf2e-9722-446e-aef7-7b35e3df68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23825 NodeManager\n",
      "22758 DataNode\n",
      "23414 ResourceManager\n",
      "24118 Jps\n",
      "23065 SecondaryNameNode\n",
      "22494 NameNode\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0671ae-5c0b-445d-bd31-c7ddf38493ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 10:35:13.592283, p21513, th128498892310016, WARNING the number of nodes in pipeline is 1 [nmit(127.0.0.1)], is less than the expected number of replica 3 for block [block pool ID: BP-835869387-127.0.1.1-1727256748010 block ID 1073742159_1335] file /home/hadoop/AAPL_IBM_open.csv\n"
     ]
    }
   ],
   "source": [
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem(host='localhost', port=9000)\n",
    "with hdfs.open('/home/hadoop/AAPL_IBM_open.csv', 'wb') as f:\n",
    "    data['Open'].to_csv(f,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d4045a8-6332-4126-a07f-921e997c545d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hadoop/AAPL_IBM_open.csv', '/home/hadoop/user']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.ls('/home/hadoop/.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4aea883-8dca-40e7-95bb-fe7e065ba2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stock_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%file stock_analysis.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "class StockAnalysis(MRJob):\n",
    "\n",
    "   def mapper(self, key, value):\n",
    "      date, apple_open, samsung_open = value.split(',')\n",
    "      #print(value, file=sys.stderr)\n",
    "      year = date[:4]\n",
    "      month = date[5:7]\n",
    "      if (month=='10' or month=='11' or month=='12'):\n",
    "         apple_key = 'apple_%s' % year\n",
    "         samsung_key = 'samsung_%s' % year\n",
    "         yield(apple_key, float(apple_open))\n",
    "         yield(samsung_key, float(samsung_open))\n",
    "      \n",
    "   def reducer(self, key, values):\n",
    "      yield(key, max(values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   StockAnalysis.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2457e42-46aa-4802-a477-ee93b7733996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 2, in <module>\n",
      "    from mrjob.job import MRJob\n",
      "ModuleNotFoundError: No module named 'mrjob'\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs://AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b1aff9-de46-48c3-be96-f4b65a96188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages (from mrjob) (6.0.1)\n",
      "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
      "Installing collected packages: mrjob\n",
      "Successfully installed mrjob-0.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d66d03-db9b-4251-a196-b60f882a8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hadoop/hadoop-3.4.0/bin...\n",
      "Found hadoop binary: /home/hadoop/hadoop-3.4.0/bin/hadoop\n",
      "STDERR: 2024-12-16 10:30:38,963 WARN fs.FileSystem: Failed to initialize filesystem hdfs://AAPL_IBM_open.csv: java.io.IOException: Incomplete HDFS URI, no host: hdfs://AAPL_IBM_open.csv\n",
      "STDERR: ls: Incomplete HDFS URI, no host: hdfs://AAPL_IBM_open.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/hadoop.py\", line 307, in exists\n",
      "    return_code = self.invoke_hadoop(\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/hadoop.py\", line 183, in invoke_hadoop\n",
      "    raise CalledProcessError(proc.returncode, args)\n",
      "subprocess.CalledProcessError: Command '['/home/hadoop/hadoop-3.4.0/bin/hadoop', 'fs', '-ls', 'hdfs://AAPL_IBM_open.csv']' returned non-zero exit status 1.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 24, in <module>\n",
      "    StockAnalysis.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1145, in _check_input_path\n",
      "    if not self.fs.exists(path):\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/composite.py\", line 142, in exists\n",
      "    return self._do('exists', path_glob)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/composite.py\", line 124, in _do\n",
      "    return self._handle(name, path, path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/composite.py\", line 110, in _handle\n",
      "    return getattr(fs, name)(*args, **kwargs)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/fs/hadoop.py\", line 314, in exists\n",
      "    raise IOError(\"Could not check path %s\" % path_glob)\n",
      "OSError: Could not check path hdfs://AAPL_IBM_open.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs://AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7076c7c7-431a-4ae4-a5c2-58d2251d1b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 24, in <module>\n",
      "    StockAnalysis.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1146, in _check_input_path\n",
      "    raise IOError(\n",
      "OSError: Input path hdfs:/AAPL_IBM_open.csv does not exist!\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs:/AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04782331-afd2-43ea-8a50-1e519d73b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 24, in <module>\n",
      "    StockAnalysis.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1146, in _check_input_path\n",
      "    raise IOError(\n",
      "OSError: Input path hdfs:/home/hadoop/AAPL_IBM_open.csv does not exist!\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs:/home/hadoop/AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e02743-f4ce-4ffc-9eda-8d5b549e6480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 24, in <module>\n",
      "    StockAnalysis.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1146, in _check_input_path\n",
      "    raise IOError(\n",
      "OSError: Input path hdfs:/home/hadoop/user/AAPL_IBM_open.csv does not exist!\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs:/home/hadoop/user/AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1abae4-faf6-40bb-a83a-875dc53953ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/stock_analysis.py\", line 24, in <module>\n",
      "    StockAnalysis.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 500, in run\n",
      "    self._check_input_paths()\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1133, in _check_input_paths\n",
      "    self._check_input_path(path)\n",
      "  File \"/home/nmit/anaconda3/envs/stock/lib/python3.9/site-packages/mrjob/runner.py\", line 1146, in _check_input_path\n",
      "    raise IOError(\n",
      "OSError: Input path hdfs:/home/hadoop/APL_IBM_open.csv does not exist!\n"
     ]
    }
   ],
   "source": [
    "!python3 stock_analysis.py -r hadoop hdfs:/home/hadoop/APL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "732a0c26-4337-4ce2-8b53-9c10e08565a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/hadoop/hadoop-3.4.0/bin...\n",
      "Found hadoop binary: /home/hadoop/hadoop-3.4.0/bin/hadoop\n",
      "Using Hadoop version 3.4.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/hadoop-3.4.0...\n",
      "Found Hadoop streaming jar: /home/hadoop/hadoop-3.4.0/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar\n",
      "Creating temp directory /tmp/stock_analysis.hadoop.20241216.051012.318261\n",
      "uploading working dir files to hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261/files/wd...\n",
      "Copying other local files to hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1775918782296022663/] [] /tmp/streamjob2970879527946789476.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1734324554330_0001\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1734324554330_0001\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1734324554330_0001\n",
      "  The url to track the job: http://nmit:8088/proxy/application_1734324554330_0001/\n",
      "  Running job: job_1734324554330_0001\n",
      "  Job job_1734324554330_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1734324554330_0001 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=135186\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=714\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=47668\n",
      "\t\tFILE: Number of bytes written=1036378\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=135392\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=714\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2262016\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1091584\n",
      "\t\tTotal time spent by all map tasks (ms)=2209\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2209\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1066\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1066\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2209\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1066\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=76\n",
      "\t\tInput split bytes=206\n",
      "\t\tMap input records=2768\n",
      "\t\tMap output bytes=44874\n",
      "\t\tMap output materialized bytes=47674\n",
      "\t\tMap output records=1394\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=313008128\n",
      "\t\tPeak Map Virtual memory (bytes)=2641993728\n",
      "\t\tPeak Reduce Physical memory (bytes)=361836544\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2643185664\n",
      "\t\tPhysical memory (bytes) snapshot=985268224\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=1394\n",
      "\t\tReduce output records=22\n",
      "\t\tReduce shuffle bytes=47674\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2788\n",
      "\t\tTotal committed heap usage (bytes)=1367867392\n",
      "\t\tVirtual memory (bytes) snapshot=7925956608\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261/output\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261/output...\n",
      "\"apple_2009\"\t7.611785888671875\n",
      "\"apple_2010\"\t11.650713920593262\n",
      "\"apple_2011\"\t15.062856674194336\n",
      "\"apple_2012\"\t23.97321319580078\n",
      "\"apple_2013\"\t20.451786041259766\n",
      "\"apple_2014\"\t29.8174991607666\n",
      "\"apple_2015\"\t30.782499313354492\n",
      "\"apple_2016\"\t29.545000076293945\n",
      "\"apple_2017\"\t43.77750015258789\n",
      "\"apple_2018\"\t57.69499969482422\n",
      "\"apple_2019\"\t72.77999877929688\n",
      "\"samsung_2009\"\t126.58699798583984\n",
      "\"samsung_2010\"\t140.27725219726562\n",
      "\"samsung_2011\"\t185.12428283691406\n",
      "\"samsung_2012\"\t201.86424255371094\n",
      "\"samsung_2013\"\t178.28871154785156\n",
      "\"samsung_2014\"\t181.55831909179688\n",
      "\"samsung_2015\"\t145.75526428222656\n",
      "\"samsung_2016\"\t161.53919982910156\n",
      "\"samsung_2017\"\t154.92352294921875\n",
      "\"samsung_2018\"\t147.2275390625\n",
      "\"samsung_2019\"\t139.1873779296875\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/stock_analysis.hadoop.20241216.051012.318261...\n",
      "Removing temp directory /tmp/stock_analysis.hadoop.20241216.051012.318261...\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/hadoop/stock_analysis.py -r hadoop hdfs:///home/hadoop/AAPL_IBM_open.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6f1f7-a0ec-4166-868a-2ee36b07896c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
